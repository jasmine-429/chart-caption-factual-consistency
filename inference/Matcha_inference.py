import os
import json
import torch
from PIL import Image
from tqdm import tqdm
from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration

# ===== æ¨¡å‹é…ç½® =====
os.environ["CUDA_VISIBLE_DEVICES"] = "7"
model_id = "google/matcha-chart2text-statista"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

processor = Pix2StructProcessor.from_pretrained(model_id)
model = Pix2StructForConditionalGeneration.from_pretrained(model_id).to(device).eval()
query = "Please describe the chart."

# ===== è·¯å¾„é…ç½® =====
Img_root = "/data/jguo376/project/dataset/ChartX_dataset/ChartX"

dataset_root = "/data/jguo376/project/dataset/test_dataset/ChartX/test_eva_data/data"
output_root="/data/jguo376/project/model/matcha"
input_json = os.path.join(dataset_root, "eva_test.json")
output_json = os.path.join(output_root, "matcha_caption_chartx_eva.json")

# ===== æ§åˆ¶å¤„ç†æ¡æ•° =====
max_test = None  # å¯è®¾ç½®ä¸ºæ•´æ•°ï¼Œå¦‚ 100ï¼Œä»…å¤„ç†å‰100æ¡ï¼›Noneè¡¨ç¤ºå¤„ç†å…¨éƒ¨
save_every = 20  # æ¯å¤„ç†Næ¡ä¿å­˜ä¸€æ¬¡

# ===== è¯»å–è¾“å…¥æ•°æ® =====
with open(input_json, "r", encoding="utf-8") as f:
    all_data = json.load(f)

if max_test is not None:
    all_data = all_data[:max_test]

# ===== åŠ è½½å·²å¤„ç†è®°å½•ï¼ˆæ–­ç‚¹ä¿æŠ¤ï¼‰=====
results = []
processed_imgs = set()
if os.path.exists(output_json):
    with open(output_json, "r", encoding="utf-8") as f:
        results = json.load(f)
        processed_imgs = {item["img"] for item in results}
    print(f"ğŸ” å·²åŠ è½½ {len(results)} æ¡å†å²ç»“æœï¼Œå°†è·³è¿‡å·²å¤„ç†é¡¹")

# ===== æ¨ç†å‡½æ•° =====
def generate_caption(image_path, query):
    image = Image.open(image_path).convert("RGB")
    inputs = processor(images=image, text=query, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=512)
    caption = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return caption.replace("\x0A", "").strip()

# ===== æ‰¹é‡å¤„ç† =====
new_results = []
for idx, item in enumerate(tqdm(all_data, desc="Generating captions")):
    img_key = item["img"]
    if img_key in processed_imgs:
        continue

    relative_path = item["img"]
    image_path = os.path.join(Img_root, relative_path.replace("./", "")) if not os.path.isabs(relative_path) else relative_path

    if not os.path.exists(image_path):
        caption = f"[ERROR] Image not found: {image_path}"
    else:
        try:
            caption = generate_caption(image_path, query)
        except Exception as e:
            caption = f"[ERROR] {str(e)}"

    item["model_name"] = model_id
    item["img"] = image_path  # æ›¿æ¢ä¸ºç»å¯¹è·¯å¾„
    item["generated_caption"] = caption

    results.append(item)
    new_results.append(item)
    processed_imgs.add(img_key)

    # ===== æ¯20æ¡ä¿å­˜ä¸€æ¬¡ =====
    if len(new_results) >= save_every:
        with open(output_json, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"å·²ä¿å­˜ {len(results)} æ¡ç»“æœï¼‰")
        new_results.clear()

# ===== æœ€ç»ˆä¿å­˜ =====
if new_results:
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"æœ€ç»ˆä¿å­˜ {len(results)} æ¡ç»“æœ")

print(f"ä»»åŠ¡å®Œæˆï¼æ€»å…±ç”Ÿæˆ {len(results)} æ¡ captionï¼Œè¾“å‡ºè·¯å¾„ï¼š{output_json}")
